{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIitjuDXnPom"
   },
   "source": [
    "\n",
    "• Create a new MLP with any given number of inputs, any number of  outputs (can be sigmoidal or linear), and any number of hidden units (sigmoidal/tanh) in a single layer. \n",
    "• Initialise the weights of the MLP to small random values \n",
    "• Predict the outputs corresponding to an input vector \n",
    "• Implement learning by backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gOYY2ErlKAIl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class MLP:\n",
    "    def __init__(self, NI, NH, NO):\n",
    "        # intiatization of attributes \n",
    "        self.Z1 = np.array\n",
    "        self.Z2 = np.array\n",
    "        self.H = np.array\n",
    "        self.O = np.array\n",
    "        self.numberofinputlayer = NI\n",
    "        self.numberofHL = NH\n",
    "        self.numberofOL = NO\n",
    "        self.W1 = np.array\n",
    "        self.W2 = np.array\n",
    "        self.dW1 = np.array\n",
    "        self.dW2 = np.array\n",
    "\n",
    "\n",
    "    def randomise(self):\n",
    "        # initializing W1 and W2 to small random values, \n",
    "        #various range from zero to at least one, for both layers from input to hidden unit\n",
    "        # and from hidden units to outputs\n",
    "        self.W1 = np.array((np.random.uniform(0, 1, (self.numberofinputlayer, self.numberofHL))).tolist())\n",
    "        self.W2 = np.array((np.random.uniform(0, 1, (self.numberofHL, self.numberofOL))).tolist())\n",
    "\n",
    "        # set dW2 and dW2 to all zeroes\n",
    "        self.dW1 = np.dot(self.W1, 0)\n",
    "        self.dW2 = np.dot(self.W2, 0)\n",
    "\n",
    "    def sig(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def der_sig(self, x):\n",
    "        return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (2 / (1 + np.exp(x * -2))) - 1\n",
    "\n",
    "    def tanh_der(self, x):\n",
    "        return 1 - (np.power(self.tanh(x), 2))\n",
    "#Forward pass. Input vector I is processed to produce an output,which is stored in O[] \n",
    "    def forward(self, I, activation):\n",
    "        if activation == 'sig':\n",
    "            self.Z1 = np.dot(I, self.W1)\n",
    "            self.H = self.sig(self.Z1)\n",
    "            self.Z2 = np.dot(self.H, self.W2)\n",
    "            self.O = self.sig(self.Z2)\n",
    "\n",
    "        elif activation == 'tanh':\n",
    "            self.Z1 = np.dot(I, self.W1)\n",
    "            self.H = self.tanh(self.Z1)\n",
    "            self.Z2 = np.dot(self.H, self.W2)\n",
    "            self.O = self.Z2\n",
    "\n",
    "        return self.O\n",
    "#doubebackwards - Backwards pass. Target t is compared with output O, deltas are computed\n",
    "#for the upper layer, and are multiplied by the inputs to the layer (the\n",
    "#values in H) to produce the weight updates which are stored in dW2 (added to it)\n",
    "    def backwards(self, I, target, activation):\n",
    "        output_error = np.subtract(target, self.O)\n",
    "        if activation == 'sig':\n",
    "            activation_upper = self.der_sig(self.Z2)\n",
    "            activation_lower = self.der_sig(self.Z1)\n",
    "        elif activation == 'tanh':\n",
    "            activation_upper = self.tanh_der(self.Z2)\n",
    "            activation_lower = self.tanh_der(self.Z1)\n",
    "        dw2_a = np.multiply(output_error, activation_upper)\n",
    "        self.dW2 = np.dot(self.H.T, dw2_a)\n",
    "        dw1_a = np.multiply(np.dot(dw2_a, self.W2.T), activation_lower)\n",
    "        self.dW1 = np.dot(I.T, dw1_a)\n",
    "        return np.mean(np.abs(output_error))\n",
    "#update_weight - this simply does adjusting the weight \n",
    "#(component by component, i.e. within for loops)\n",
    "    def update_weights(self, Lr):\n",
    "        self.W1 = np.add(self.W1, Lr * self.dW1)\n",
    "        self.W2 = np.add(self.W2, Lr * self.dW2)\n",
    "        self.dW1 = np.array\n",
    "        self.dW2 = np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGz8fH1UbT-u"
   },
   "source": [
    "#Q1 :1. Train an MLP with 2 inputs, 3-4+ hidden units and one output on  the following examples (XOR function): \n",
    "#((0, 0), 0) \n",
    "#((0, 1), 1) \n",
    "#((1, 0), 1) \n",
    "#((1, 1), 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "24PpUMOQURcW"
   },
   "outputs": [],
   "source": [
    "fname1 = open(\"Q1_XOR.txt\", \"w\")\n",
    "print(\"1. XOR test\\n\", file=fname1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q-vHvRmIUqU1"
   },
   "outputs": [],
   "source": [
    "def q1xor(max_epochs, LR, NH):\n",
    "    Iput = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    Out = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    NI = 2\n",
    "    NO = 1\n",
    "    obj1 = MLP(NI, NH, NO)\n",
    "\n",
    "    obj1.randomise()\n",
    "\n",
    "    print('Epochs = ' + str(max_epochs), file=fname1)\n",
    "\n",
    "    print('Learning rate = ' + str(LR), file=fname1)\n",
    "\n",
    "    print('Hidden units = ' + str(NH) + '\\n\\n', file=fname1)\n",
    "\n",
    "    print('Before Training:\\n', file=fname1)\n",
    "\n",
    "    for i in range(len(Iput)):\n",
    "        obj1.forward(Iput[i], 'sig')\n",
    "        print('Target:\\t {}  Output:\\t {}'.format(str(Out[i]), str(obj1.O)), file=fname1)\n",
    "\n",
    "    print('\\nTraining:\\n', file=fname1)\n",
    "\n",
    "\n",
    "    # training\n",
    "    for i in range(max_epochs):\n",
    "        obj1.forward(Iput, 'sig')\n",
    "        error = obj1.backwards(Iput, Out, 'sig')\n",
    "        obj1.update_weights(LR)\n",
    "\n",
    "        if (i + 1) == 100:\n",
    "            print(' Error at Epoch:\\t' + str(i + 1) + '\\t\\t  is \\t' + str(error), file=fname1)\n",
    "\n",
    "        elif (i + 1) == 1000 or (i + 1) == 10000 or (i + 1) == 100000 or (i + 1) == 1000000:\n",
    "            print(' Error at Epoch:\\t' + str(i + 1) + '\\t  is \\t' + str(error), file=fname1)\n",
    "\n",
    "\n",
    "    print('\\nAfter Training :\\n', file=fname1)\n",
    "\n",
    "\n",
    "    # get accuracy after training\n",
    "    accuracy = float()\n",
    "    for i in range(len(Iput)):\n",
    "        obj1.forward(Iput[i], 'sig')\n",
    "        print('Target:\\t {}  Output:\\t {}'.format(str(Out[i]), str(obj1.O)), file=fname1)\n",
    "\n",
    "        if (Out[i][0] == 0):\n",
    "            accuracy += 1 - obj1.O[0]\n",
    "        elif (Out[i][0] == 1):\n",
    "            accuracy += obj1.O[0]\n",
    "    print('\\n\\nAccuracy = ' + str(accuracy / 4), file=fname1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZoEKxFi_i8q3"
   },
   "outputs": [],
   "source": [
    "epochs = [900000] # initialize number of epoch\n",
    "LR = [0.05,0.25,0.5,0.75,1.0]  # Assign different learning Rate\n",
    "num_hidden = 7\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "    for j in range(len(LR)):\n",
    "        q1xor(epochs[i], LR[j], num_hidden)  # Xor function call\n",
    "        print('\\n#*****************************************#\\n', file=fname1)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
