{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXq-8tVtUHq1"
   },
   "source": [
    "# \n",
    ">- Create a new MLP with any given number of inputs, any number of outputs (can be sigmoidal or linear), and any number of hidden units (sigmoidal/tanh) in a single layer. \n",
    ">- Initialise the weights of the MLP to small random values \n",
    ">- Predict the outputs corresponding to an input vector\n",
    ">- Implement learning by backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1670865013507,
     "user": {
      "displayName": "Sharmila Chopade",
      "userId": "02341898456798771098"
     },
     "user_tz": -330
    },
    "id": "gOYY2ErlKAIl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class MLP:\n",
    "    def __init__(self, NI, NH, NO):\n",
    "        # intiatization of attributes \n",
    "        self.Z1 = np.array\n",
    "        self.Z2 = np.array\n",
    "        self.H = np.array\n",
    "        self.O = np.array\n",
    "        self.numberofinputlayer = NI\n",
    "        self.numberofHL = NH\n",
    "        self.numberofOL = NO\n",
    "        self.W1 = np.array\n",
    "        self.W2 = np.array\n",
    "        self.dW1 = np.array\n",
    "        self.dW2 = np.array\n",
    "\n",
    "\n",
    "    def randomise(self):\n",
    "        # initializing W1 and W2 to small random values, \n",
    "        #various range from zero to at least one, for both layers from input to hidden unit\n",
    "        # and from hidden units to outputs\n",
    "        self.W1 = np.array((np.random.uniform(0, 1, (self.numberofinputlayer, self.numberofHL))).tolist())\n",
    "        self.W2 = np.array((np.random.uniform(0, 1, (self.numberofHL, self.numberofOL))).tolist())\n",
    "\n",
    "        # set dW2 and dW2 to all zeroes\n",
    "        self.dW1 = np.dot(self.W1, 0)\n",
    "        self.dW2 = np.dot(self.W2, 0)\n",
    "\n",
    "    def sig(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def der_sig(self, x):\n",
    "        return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (2 / (1 + np.exp(x * -2))) - 1\n",
    "\n",
    "    def tanh_der(self, x):\n",
    "        return 1 - (np.power(self.tanh(x), 2))\n",
    "#Forward pass. Input vector I is processed to produce an output,which is stored in O[] \n",
    "    def forward(self, I, activation):\n",
    "        if activation == 'sig':\n",
    "            self.Z1 = np.dot(I, self.W1)\n",
    "            self.H = self.sig(self.Z1)\n",
    "            self.Z2 = np.dot(self.H, self.W2)\n",
    "            self.O = self.sig(self.Z2)\n",
    "\n",
    "        elif activation == 'tanh':\n",
    "            self.Z1 = np.dot(I, self.W1)\n",
    "            self.H = self.tanh(self.Z1)\n",
    "            self.Z2 = np.dot(self.H, self.W2)\n",
    "            self.O = self.Z2\n",
    "\n",
    "        return self.O\n",
    "#doubebackwards - Backwards pass. Target t is compared with output O, deltas are computed\n",
    "#for the upper layer, and are multiplied by the inputs to the layer (the\n",
    "#values in H) to produce the weight updates which are stored in dW2 (added to it)\n",
    "    def backwards(self, I, target, activation):\n",
    "        output_error = np.subtract(target, self.O)\n",
    "        if activation == 'sig':\n",
    "            activation_upper = self.der_sig(self.Z2)\n",
    "            activation_lower = self.der_sig(self.Z1)\n",
    "        elif activation == 'tanh':\n",
    "            activation_upper = self.tanh_der(self.Z2)\n",
    "            activation_lower = self.tanh_der(self.Z1)\n",
    "        dw2_a = np.multiply(output_error, activation_upper)\n",
    "        self.dW2 = np.dot(self.H.T, dw2_a)\n",
    "        dw1_a = np.multiply(np.dot(dw2_a, self.W2.T), activation_lower)\n",
    "        self.dW1 = np.dot(I.T, dw1_a)\n",
    "        return np.mean(np.abs(output_error))\n",
    "#update_weight - this simply does adjusting the weight \n",
    "#(component by component, i.e. within for loops)\n",
    "    def update_weights(self, Lr):\n",
    "        self.W1 = np.add(self.W1, Lr * self.dW1)\n",
    "        self.W2 = np.add(self.W2, Lr * self.dW2)\n",
    "        self.dW1 = np.array\n",
    "        self.dW2 = np.array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gRxNeBqXn3q"
   },
   "source": [
    "# Q2:\n",
    "Generate 500 vectors containing 4 components each. The value of \n",
    "each component should be a random number between -1 and 1. These will \n",
    "be your input vectors. The corresponding output for each vector should \n",
    "be the sin() of a combination of the components. Specifically, for \n",
    "inputs: \n",
    "[x1 x2 x3 x4] the (single component) \n",
    "output should be: \n",
    "sin(x1-x2+x3-x4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1670865013508,
     "user": {
      "displayName": "Sharmila Chopade",
      "userId": "02341898456798771098"
     },
     "user_tz": -330
    },
    "id": "SrMc6mNAUHq7"
   },
   "outputs": [],
   "source": [
    "\n",
    "fname = open(\"Q2_SIN.txt\", \"w\")\n",
    "print(\"Q2 out put \\n\", file=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670865013508,
     "user": {
      "displayName": "Sharmila Chopade",
      "userId": "02341898456798771098"
     },
     "user_tz": -330
    },
    "id": "N8H1qNTsX0ZM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlpq2(max_epochs, Lr, NH):\n",
    "    ip = []\n",
    "    out = []\n",
    "\n",
    "    print('Number of Epoches is  = ' + str(max_epochs), file=fname)\n",
    "\n",
    "    print('The Learning Rate would be = ' + str(Lr), file=fname)\n",
    "\n",
    "    print('The Number of Hidden units are = ' + str(NH) + '\\n\\n', file=fname)\n",
    "\n",
    "    print('The Pre Training results are:', file=fname)\n",
    "\n",
    "    for i in range(500):\n",
    "        vtr = list(np.random.uniform(-1.0, 1.0, 4))\n",
    "        vtr = [float(vtr[0]), float(vtr[1]), float(vtr[2]), float(vtr[3])]\n",
    "\n",
    "        ip.append(vtr)\n",
    "\n",
    "    ip = np.array(ip)\n",
    "\n",
    "    for i in range(500):\n",
    "        out.append(np.sin([ip[i][0] - ip[i][1] + ip[i][2] - ip[i][3]]))\n",
    "\n",
    "    num_ip = 4\n",
    "    num_out = 1\n",
    "    mlpobj2 = MLP(num_ip, NH, num_out) \n",
    "\n",
    "    mlpobj2.randomise()\n",
    "\n",
    "    for i in range(400):\n",
    "        mlpobj2.forward(ip[i], 'sigmoid')\n",
    "        print(' Target:\\t {}  Output:\\t {}'.format(str(out[i]), str(mlpobj2.O)), file=fname)\n",
    "\n",
    "    print('\\nTraining Results:\\n', file=fname)\n",
    "\n",
    "\n",
    "    # training\n",
    "    for i in range(max_epochs):\n",
    "        #forward - used to directly call a method in the class when an instance name is called\n",
    "        mlpobj2.forward(ip[:400], 'tanh')\n",
    "        #backwardused to compute the gradient during the backward pass in a neural network. \n",
    "        error =mlpobj2.backwards(ip[:400], out[:400], 'tanh')\n",
    "        mlpobj2.update_weights(Lr)\n",
    "\n",
    "        if (i + 1) == 100:\n",
    "            print(' The accures Error at Epoch:\\t' + str(i + 1) + '\\t\\t  is \\t' + str(error), file=fname)\n",
    "\n",
    "        elif (i + 1) == 1000 or (i + 1) == 10000 or (i + 1) == 100000 or (i + 1) == 1000000:\n",
    "            print('The accures Error at Epoch:\\t' + str(i + 1) + '\\t  is \\t' + str(error), file=fname)\n",
    "\n",
    "    print('\\nAfter Training :\\n', file=fname)\n",
    "\n",
    "    # testing\n",
    "    diff = 0\n",
    "    for i in range(400, len(ip)):\n",
    "        mlpobj2.forward(ip[i], 'tanh')\n",
    "        print(' Target:\\t{}\\t Output:\\t {}'.format(str(out[i]), str(mlpobj2.O)), file=fname)\n",
    "        diff = diff + np.abs(out[i][0] -mlpobj2.O[0])\n",
    "\n",
    "    accuracy = 1-(diff/100)\n",
    "    print('\\nThe Accuracy of the model = ' + str(accuracy), file=fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 719597,
     "status": "ok",
     "timestamp": 1670865733101,
     "user": {
      "displayName": "Sharmila Chopade",
      "userId": "02341898456798771098"
     },
     "user_tz": -330
    },
    "id": "XsnQJ9f0Y2fh",
    "outputId": "a8399578-3198-449f-8522-0479262373e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Q2  with epochs,learning rate and number of hidden layers\n"
     ]
    }
   ],
   "source": [
    "echs = [95000]\n",
    "Lr = [0.1, 0.01, 0.001, 0.0001]\n",
    "num_HL = 35\n",
    "print('Output of Q2  with epochs,learning rate and number of hidden layers')\n",
    "for j in range(len(echs)): # itrate for loop for number of epoches\n",
    "    for k in range(len(Lr)):\n",
    "        mlpq2(echs[j], Lr[k], num_HL)\n",
    "        print('\\n*********************************************************************************\\n', file=fname)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
