{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PBaCqICTif_"
   },
   "source": [
    "#• Create a new MLP with any given number of inputs, any number of outputs (can be sigmoidal or linear), and any number of hidden units (sigmoidal/tanh) in a single layer. • Initialise the weights of the MLP to small random values • Predict the outputs corresponding to an input vector • Implement learning by backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOYY2ErlKAIl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class MLP:\n",
    "    def __init__(self, NI, NH, NO):\n",
    "        # intiatization of attributes \n",
    "        self.Z1 = np.array\n",
    "        self.Z2 = np.array\n",
    "        self.H = np.array\n",
    "        self.O = np.array\n",
    "        self.numberofinputlayer = NI\n",
    "        self.numberofHL = NH\n",
    "        self.numberofOL = NO\n",
    "        self.W1 = np.array\n",
    "        self.W2 = np.array\n",
    "        self.dW1 = np.array\n",
    "        self.dW2 = np.array\n",
    "\n",
    "\n",
    "    def randomise(self):\n",
    "        # initializing W1 and W2 to small random values, \n",
    "        #various range from zero to at least one, for both layers from input to hidden unit\n",
    "        # and from hidden units to outputs\n",
    "        self.W1 = np.array((np.random.uniform(0, 1, (self.numberofinputlayer, self.numberofHL))).tolist())\n",
    "        self.W2 = np.array((np.random.uniform(0, 1, (self.numberofHL, self.numberofOL))).tolist())\n",
    "\n",
    "        # set dW2 and dW2 to all zeroes\n",
    "        self.dW1 = np.dot(self.W1, 0)\n",
    "        self.dW2 = np.dot(self.W2, 0)\n",
    "\n",
    "    def sig(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def der_sig(self, x):\n",
    "        return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (2 / (1 + np.exp(x * -2))) - 1\n",
    "\n",
    "    def tanh_der(self, x):\n",
    "        return 1 - (np.power(self.tanh(x), 2))\n",
    "#Forward pass. Input vector I is processed to produce an output,which is stored in O[] \n",
    "    def forward(self, I, activation):\n",
    "        if activation == 'sig':\n",
    "            self.Z1 = np.dot(I, self.W1)\n",
    "            self.H = self.sig(self.Z1)\n",
    "            self.Z2 = np.dot(self.H, self.W2)\n",
    "            self.O = self.sig(self.Z2)\n",
    "\n",
    "        elif activation == 'tanh':\n",
    "            self.Z1 = np.dot(I, self.W1)\n",
    "            self.H = self.tanh(self.Z1)\n",
    "            self.Z2 = np.dot(self.H, self.W2)\n",
    "            self.O = self.Z2\n",
    "\n",
    "        return self.O\n",
    "#doubebackwards - Backwards pass. Target t is compared with output O, deltas are computed\n",
    "#for the upper layer, and are multiplied by the inputs to the layer (the\n",
    "#values in H) to produce the weight updates which are stored in dW2 (added to it)\n",
    "    def backwards(self, I, target, activation):\n",
    "        output_error = np.subtract(target, self.O)\n",
    "        if activation == 'sig':\n",
    "            activation_upper = self.der_sig(self.Z2)\n",
    "            activation_lower = self.der_sig(self.Z1)\n",
    "        elif activation == 'tanh':\n",
    "            activation_upper = self.tanh_der(self.Z2)\n",
    "            activation_lower = self.tanh_der(self.Z1)\n",
    "        dw2_a = np.multiply(output_error, activation_upper)\n",
    "        self.dW2 = np.dot(self.H.T, dw2_a)\n",
    "        dw1_a = np.multiply(np.dot(dw2_a, self.W2.T), activation_lower)\n",
    "        self.dW1 = np.dot(I.T, dw1_a)\n",
    "        return np.mean(np.abs(output_error))\n",
    "#update_weight - this simply does adjusting the weight \n",
    "#(component by component, i.e. within for loops)\n",
    "    def update_weights(self, Lr):\n",
    "        self.W1 = np.add(self.W1, Lr * self.dW1)\n",
    "        self.W2 = np.add(self.W2, Lr * self.dW2)\n",
    "        self.dW1 = np.array\n",
    "        self.dW2 = np.array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qBXEOpKXyyx"
   },
   "source": [
    "### Q3 Letter Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOCvf2iJSSnA"
   },
   "outputs": [],
   "source": [
    "fname3 = open(\"Q3_letter_recognition.txt\", \"w\")\n",
    "print(\"Q3 LETTER Prediction using MLP \\n\", file=fname3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqiC1uEiR6L5"
   },
   "outputs": [],
   "source": [
    "def letter(max_epochs, learning_rate, num_hidden):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    Iput3 = []\n",
    "    out3 = []\n",
    "    data_output = []\n",
    "    columns = [\"letter\", \"x-box\", \"y-box\", \"width\", \"height\", \"onpix\", \"x-bar\", \"y-bar\", \"x2bar\", \"y2bar\", \"xybar\",\n",
    "               \"x2ybr\", \"xy2br\", \"x-ege\", \"xegvy\", \"y-ege\", \"yegvx\"]\n",
    "\n",
    "    df = pd.read_csv(\"letter-recognition.data\", names=columns)\n",
    "    data_output = df[\"letter\"]\n",
    "\n",
    "    for i in range(len(data_output)):\n",
    "        out3.append(ord(str(data_output[i])) - ord('A'))\n",
    "\n",
    "    Iput3 = df.drop([\"letter\"], axis=1)\n",
    "    Iput3 = np.array(Iput3)\n",
    "    Iput3 = Iput3 / 15  # normalization\n",
    "\n",
    "    # train set\n",
    "    Iput3_train = Iput3[:16000]\n",
    "    categorical_y = np.zeros((16000, 26))\n",
    "    for i, l in enumerate(out3[:16000]):\n",
    "        categorical_y[i][l] = 1\n",
    "    out3_train = categorical_y\n",
    "\n",
    "    # test set\n",
    "    Iput3_test = Iput3[16000:]\n",
    "    num_Iput3 = 16\n",
    "    num_out3 = 26\n",
    "\n",
    "    obj3 = MLP(num_Iput3, num_hidden, num_out3)\n",
    "    obj3.randomise()\n",
    "    print('The number Epochs = ' + str(max_epochs), file=fname3)\n",
    "\n",
    "    print('The Learning rate is = ' + str(learning_rate), file=fname3)\n",
    "\n",
    "    print('The number of Hidden Layers = ' + str(num_hidden) + '\\n\\n', file=fname3)\n",
    "\n",
    "    for i in range(0, max_epochs):\n",
    "        obj3.forward(Iput3_train, 'tanh')\n",
    "        error = obj3.backwards(Iput3_train, out3_train, 'tanh')\n",
    "        obj3.update_weights(learning_rate)\n",
    "\n",
    "        if (i + 1) % (max_epochs / 20) == 0:\n",
    "            print(' Error at Epoch:\\t' + str(i + 1) + '\\t  is \\t' + str(error), file=fname3)\n",
    "\n",
    "    # testing\n",
    "    def to_character0(output_vector):\n",
    "        list_of_vectors = list(output_vector)\n",
    "        a = list_of_vectors.index(max(list_of_vectors))\n",
    "        return chr(a + ord('A'))\n",
    "\n",
    "    prediction = []\n",
    "    for i in range(4000):\n",
    "        obj3.forward(Iput3_test[i], 'tanh')\n",
    "        prediction.append(to_character0(obj3.O))\n",
    "\n",
    "    def to_character(n):\n",
    "        return chr(int(n) + ord('A'))\n",
    "\n",
    "    correct_letter = {to_character(i): 0 for i in range(26)}\n",
    "    letter_num = {to_character(i): 0 for i in range(26)}\n",
    "\n",
    "    print('\\n#**********************************************************************************#\\n', file=fname3)\n",
    "\n",
    "    for i, _ in enumerate(data_output[16000:]):\n",
    "        letter_num[data_output[16000 + i]] += 1\n",
    "        # predictions\n",
    "        if i % 300 == 0:\n",
    "            print('Expected: {} | Output: {}'.format(data_output[16000 + i], prediction[i]), file=fname3)\n",
    "        if data_output[16000 + i] == prediction[i]:\n",
    "            correct_letter[prediction[i]] += 1\n",
    "\n",
    "    print('\\n#**********************************************************************************#\\n', file=fname3)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = sum(correct_letter.values()) / len(prediction)\n",
    "    print('Test sample size: {} | Correctly predicted sample size: {}'.format(len(prediction),\n",
    "                                                                              sum(correct_letter.values())),\n",
    "          file=fname3)\n",
    "    print('The Accuracy of preiction of letters: %.3f' % accuracy, file=fname3)\n",
    "\n",
    "    # Performance of prediction of each letter\n",
    "    print('\\n#**********************************************************************************#\\n', file=fname3)\n",
    "\n",
    "    for k, v in letter_num.items():\n",
    "        print('{} => Number of occurrences in the sample: {}'\n",
    "              ' | Number of correct predictions: {}'\n",
    "              ' | Accuracy: {}'.format(k, v, correct_letter[k], correct_letter[k] / v),\n",
    "              file=fname3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npOouh6xZvQ6"
   },
   "outputs": [],
   "source": [
    "#Model calling \n",
    "epochs = [1000000]\n",
    "LR = [0.000005]\n",
    "num_hidden = 10\n",
    "for i in range(len(epochs)):\n",
    "    for j in range(len(LR)):\n",
    "        print('\\n#**********************************************************************************#\\n', file=fname3)\n",
    "        letter(epochs[i], LR[j], num_hidden)\n",
    "        print('\\n#**********************************************************************************#\\n', file=fname3)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
